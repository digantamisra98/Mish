{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Funnel_activation",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "id": "QIW-dgNsj9ar"
   },
   "source": [
    "# Model Parameter and FLOP counter\n",
    "!pip install ptflops\n",
    "# Install WandB for metric logging\n",
    "!pip install wandb"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DWJZdmaBrqnY"
   },
   "source": [
    "!nvidia-smi"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_ccW_-r0ZAFP"
   },
   "source": [
    "best_prec1 = 0\n",
    "evaluate = True\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ptflops import get_model_complexity_info\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.nn.parameter import Parameter\n",
    "import wandb\n",
    "\n",
    "\n",
    "def main():\n",
    "    global best_prec1, evaluate\n",
    "\n",
    "    wandb.init(project=\"Mish\")\n",
    "\n",
    "    __all__ = [\n",
    "        \"ResNet\",\n",
    "        \"resnet20\",\n",
    "        \"resnet32\",\n",
    "        \"resnet44\",\n",
    "        \"resnet56\",\n",
    "        \"resnet110\",\n",
    "        \"resnet1202\",\n",
    "    ]\n",
    "\n",
    "    def _weights_init(m):\n",
    "        classname = m.__class__.__name__\n",
    "        # print(classname)\n",
    "        if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "            init.kaiming_normal_(m.weight)\n",
    "\n",
    "    class LambdaLayer(nn.Module):\n",
    "        def __init__(self, lambd):\n",
    "            super(LambdaLayer, self).__init__()\n",
    "            self.lambd = lambd\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.lambd(x)\n",
    "\n",
    "    class Mish(nn.Module):\n",
    "        def __init__(self, lambd):\n",
    "            super(Mish, self).__init__()\n",
    "\n",
    "        def forward(self, x):\n",
    "            return x * torch.tanh(F.softplus(x))\n",
    "\n",
    "    class BasicBlock(nn.Module):\n",
    "        expansion = 1\n",
    "\n",
    "        def __init__(self, in_planes, planes, stride=1, option=\"A\"):\n",
    "            super(BasicBlock, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(\n",
    "                in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False\n",
    "            )\n",
    "            self.bn1 = nn.BatchNorm2d(planes)\n",
    "            self.conv2 = nn.Conv2d(\n",
    "                planes, planes, kernel_size=3, stride=1, padding=1, bias=False\n",
    "            )\n",
    "            self.bn2 = nn.BatchNorm2d(planes)\n",
    "            self.act = Mish()\n",
    "\n",
    "            self.shortcut = nn.Sequential()\n",
    "            if stride != 1 or in_planes != planes:\n",
    "                if option == \"A\":\n",
    "                    \"\"\"\n",
    "                    For CIFAR10 ResNet paper uses option A.\n",
    "                    \"\"\"\n",
    "                    self.shortcut = LambdaLayer(\n",
    "                        lambda x: F.pad(\n",
    "                            x[:, :, ::2, ::2],\n",
    "                            (0, 0, 0, 0, planes // 4, planes // 4),\n",
    "                            \"constant\",\n",
    "                            0,\n",
    "                        )\n",
    "                    )\n",
    "                elif option == \"B\":\n",
    "                    self.shortcut = nn.Sequential(\n",
    "                        nn.Conv2d(\n",
    "                            in_planes,\n",
    "                            self.expansion * planes,\n",
    "                            kernel_size=1,\n",
    "                            stride=stride,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                        nn.BatchNorm2d(self.expansion * planes),\n",
    "                    )\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.act(self.bn1(self.conv1(x)))\n",
    "            out = self.bn2(self.conv2(out))\n",
    "            out += self.shortcut(x)\n",
    "            out = self.act(out)\n",
    "            return out\n",
    "\n",
    "    class ResNet(nn.Module):\n",
    "        def __init__(self, block, num_blocks, num_classes=10):\n",
    "            super(ResNet, self).__init__()\n",
    "            self.in_planes = 16\n",
    "\n",
    "            self.conv1 = nn.Conv2d(\n",
    "                3, 16, kernel_size=3, stride=1, padding=1, bias=False\n",
    "            )\n",
    "            self.bn1 = nn.BatchNorm2d(16)\n",
    "            self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "            self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "            self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "            self.linear = nn.Linear(64, num_classes)\n",
    "            self.act = Mish()\n",
    "\n",
    "            self.apply(_weights_init)\n",
    "\n",
    "        def _make_layer(self, block, planes, num_blocks, stride):\n",
    "            strides = [stride] + [1] * (num_blocks - 1)\n",
    "            layers = []\n",
    "            for stride in strides:\n",
    "                layers.append(block(self.in_planes, planes, stride))\n",
    "                self.in_planes = planes * block.expansion\n",
    "\n",
    "            return nn.Sequential(*layers)\n",
    "\n",
    "        def forward(self, x):\n",
    "            out = self.act(self.bn1(self.conv1(x)))\n",
    "            out = self.layer1(out)\n",
    "            out = self.layer2(out)\n",
    "            out = self.layer3(out)\n",
    "            out = F.avg_pool2d(out, out.size()[3])\n",
    "            out = out.view(out.size(0), -1)\n",
    "            out = self.linear(out)\n",
    "            return out\n",
    "\n",
    "    def resnet20():\n",
    "        return ResNet(BasicBlock, [3, 3, 3])\n",
    "\n",
    "    def resnet32():\n",
    "        return ResNet(BasicBlock, [5, 5, 5])\n",
    "\n",
    "    def resnet44():\n",
    "        return ResNet(BasicBlock, [7, 7, 7])\n",
    "\n",
    "    def resnet56():\n",
    "        return ResNet(BasicBlock, [9, 9, 9])\n",
    "\n",
    "    def resnet110():\n",
    "        return ResNet(BasicBlock, [18, 18, 18])\n",
    "\n",
    "    def resnet1202():\n",
    "        return ResNet(BasicBlock, [200, 200, 200])\n",
    "\n",
    "    model = resnet20()\n",
    "    wand.watch(model)\n",
    "\n",
    "    with torch.cuda.device(0):\n",
    "        flops, params = get_model_complexity_info(\n",
    "            model, (3, 224, 224), as_strings=True, print_per_layer_stat=True\n",
    "        )\n",
    "        print(\"{:<30}  {:<8}\".format(\"Computational complexity: \", flops))\n",
    "        print(\"{:<30}  {:<8}\".format(\"Number of parameters: \", params))\n",
    "\n",
    "    wandb.config.update({\"Parameters\": params, \"FLOPs\": flops})\n",
    "\n",
    "    model.cuda()\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    normalize = transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(\n",
    "            root=\"./data\",\n",
    "            train=True,\n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.RandomHorizontalFlip(),\n",
    "                    transforms.RandomCrop(32, 4),\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                ]\n",
    "            ),\n",
    "            download=True,\n",
    "        ),\n",
    "        batch_size=128,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10(\n",
    "            root=\"./data\",\n",
    "            train=False,\n",
    "            transform=transforms.Compose(\n",
    "                [\n",
    "                    transforms.ToTensor(),\n",
    "                    normalize,\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=128,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    # define loss function (criterion) and pptimizer\n",
    "    criterion = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(), 0.1, momentum=0.9, weight_decay=5e-4\n",
    "    )\n",
    "\n",
    "    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "        optimizer, milestones=[100, 150], last_epoch=0 - 1\n",
    "    )\n",
    "\n",
    "    for epoch in range(0, 200):\n",
    "        # train for one epoch\n",
    "        print(\"current lr {:.5e}\".format(optimizer.param_groups[0][\"lr\"]))\n",
    "        wandb.log({\"lr\": optimizer.param_groups[0][\"lr\"]})\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        # evaluate on validation set\n",
    "        prec1 = validate(val_loader, model, criterion, epoch)\n",
    "\n",
    "        # remember best prec@1 and save checkpoint\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "\n",
    "        is_best = prec1 > best_prec1\n",
    "        best_prec1 = max(prec1, best_prec1)\n",
    "        if epoch > 0 and epoch % 20 == 0:\n",
    "            save_checkpoint(\n",
    "                {\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"state_dict\": model.state_dict(),\n",
    "                    \"best_prec1\": best_prec1,\n",
    "                },\n",
    "                is_best,\n",
    "                filename=os.path.join(\"./\", \"vanilla_checkpoint.th\"),\n",
    "            )\n",
    "\n",
    "        save_checkpoint(\n",
    "            {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"best_prec1\": best_prec1,\n",
    "            },\n",
    "            is_best,\n",
    "            filename=os.path.join(\"./\", \"vanilla_model.th\"),\n",
    "        )\n",
    "\n",
    "    wandb.run.finish()\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Run one train epoch\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        target = target.cuda()\n",
    "        input_var = input.cuda()\n",
    "        target_var = target\n",
    "\n",
    "        # compute output\n",
    "        output = model(input_var)\n",
    "        loss = criterion(output, target_var)\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        output = output.float()\n",
    "        loss = loss.float()\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target)[0]\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(\n",
    "                \"Epoch: [{0}][{1}/{2}]\\t\"\n",
    "                \"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                \"Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\"\n",
    "                \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\"\n",
    "                \"Prec@1 {top1.val:.3f} ({top1.avg:.3f})\".format(\n",
    "                    epoch,\n",
    "                    i,\n",
    "                    len(train_loader),\n",
    "                    batch_time=batch_time,\n",
    "                    data_time=data_time,\n",
    "                    loss=losses,\n",
    "                    top1=top1,\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "def validate(val_loader, model, criterion, epoch):\n",
    "    \"\"\"\n",
    "    Run evaluation\n",
    "    \"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            target = target.cuda()\n",
    "            input_var = input.cuda()\n",
    "            target_var = target.cuda()\n",
    "\n",
    "            # compute output\n",
    "            output = model(input_var)\n",
    "            loss = criterion(output, target_var)\n",
    "\n",
    "            output = output.float()\n",
    "            loss = loss.float()\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1 = accuracy(output.data, target)[0]\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "            top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            if i % 20 == 0:\n",
    "                print(\n",
    "                    \"Test: [{0}/{1}]\\t\"\n",
    "                    \"Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\"\n",
    "                    \"Loss {loss.val:.4f} ({loss.avg:.4f})\\t\"\n",
    "                    \"Prec@1 {top1.val:.3f} ({top1.avg:.3f})\".format(\n",
    "                        i,\n",
    "                        len(val_loader),\n",
    "                        batch_time=batch_time,\n",
    "                        loss=losses,\n",
    "                        top1=top1,\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    print(\" * Prec@1 {top1.avg:.3f}\".format(top1=top1))\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"epoch\": epoch,\n",
    "            \"Top-1 accuracy\": top1.avg,\n",
    "            \"loss\": losses.avg,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return top1.avg\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename=\"checkpoint.pth.tar\"):\n",
    "    \"\"\"\n",
    "    Save the training model\n",
    "    \"\"\"\n",
    "    torch.save(state, filename)\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SgWEssqXirta"
   },
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iFSh8Jsv7SQ4"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  }
 ]
}